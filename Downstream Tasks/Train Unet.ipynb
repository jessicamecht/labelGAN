{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fffe3ea",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8228041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image,ImageOps\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics import JaccardIndex\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3305e9",
   "metadata": {},
   "source": [
    "### Setting augmentation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be9c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size of original train dataset\n",
    "train_size = 200\n",
    "#Use data augmentation\n",
    "use_augmentation = 'baseline'\n",
    "\n",
    "if use_augmentation == 'baseline':\n",
    "    model_path = 'Models/model_baseline_augmentation_' + str(train_size)+'.pt'\n",
    "elif use_augmentation:\n",
    "    model_path = 'Models/model_datasetgan_augmentation_' + str(train_size)+'.pt'\n",
    "else:\n",
    "    model_path = 'Models/model_no_augmentation_' + str(train_size)+'.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c17c9",
   "metadata": {},
   "source": [
    "### Loading model and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4f0c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Datasets\n",
    "from dataloader import get_datasets\n",
    "train_dataset,valid_dataset,test_dataset = get_datasets(train_size,use_augmentation)\n",
    "\n",
    "#Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "#Loading Model\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "                       in_channels=1, out_channels=1, init_features=32, pretrained=False)\n",
    "model = model.to(device)\n",
    "\n",
    "#Hyperparamaters\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "jaccard = JaccardIndex(num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e90ca",
   "metadata": {},
   "source": [
    "### Dice Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d6e63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dice Coefficient\n",
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    pred = torch.where(pred>=0.5,1,0)\n",
    "    iflat = pred.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a3d3ac",
   "metadata": {},
   "source": [
    "### Visualizing model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e4431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for visualization\n",
    "def visualize(data,pred,label):\n",
    "    batchsize = data.size()[0]\n",
    "    fig, axs = plt.subplots(batchsize,3,figsize=(10,10))\n",
    "    for i in range(batchsize):        \n",
    "        img_plot = data[i].permute(1,2,0).detach().cpu()\n",
    "        pred_plot = pred[i].permute(1,2,0).detach().cpu()\n",
    "        label_plot = label[i].permute(1,2,0).detach().cpu()\n",
    "        \n",
    "        axs[i,0].imshow(img_plot)\n",
    "        axs[i,0].set_title('Image')\n",
    "        \n",
    "        axs[i,1].imshow(pred_plot,cmap='gray')\n",
    "        axs[i,1].set_title('Prediction')\n",
    "        \n",
    "        axs[i,2].imshow(label_plot,cmap='gray')\n",
    "        axs[i,2].set_title('Label')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506bf19",
   "metadata": {},
   "source": [
    "### Validation and Test Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e666b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation Loop\n",
    "def validation(model,dataloader):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for i, (data, targets) in enumerate(dataloader, 0):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        pred = model(data)\n",
    "        loss = criterion(pred,targets)\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "#Test Loop\n",
    "def test(model_path,dataloader):\n",
    "    model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "                       in_channels=1, out_channels=1, init_features=32, pretrained=False)\n",
    "    weights = torch.load(model_path)\n",
    "    model.load_state_dict(weights)\n",
    "    model = model.to(device)\n",
    "    dice_scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, targets) in enumerate(dataloader, 0):\n",
    "            data = data.to(device)\n",
    "            targets = targets.type(torch.int8).to(device)\n",
    "\n",
    "            pred = model(data)\n",
    "            dice_scores.append(jaccard(pred.cpu(), targets.cpu()))\n",
    "    return np.mean(dice_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed82a2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2619a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lists to keep track of progress\n",
    "num_epochs = 100\n",
    "valid_loss = 100\n",
    "\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    losses = []\n",
    "    \n",
    "    # For each batch in the dataloader\n",
    "    for i, (data, targets) in enumerate(train_dataloader, 0):\n",
    "        \n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device).to(torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = criterion(pred,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    print(\"Epoch : %d, Loss : %2.5f\" % (epoch,np.mean(losses)))\n",
    "    \n",
    "    if (epoch+1)%10==0:\n",
    "        visualize(data[0:3],pred[0:3],targets[0:3])\n",
    "    \n",
    "    cur_loss = validation(model,valid_dataloader)\n",
    "    if cur_loss<valid_loss:\n",
    "        valid_loss = cur_loss\n",
    "        torch.save(model.state_dict(),model_path)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e28ae9",
   "metadata": {},
   "source": [
    "### Test acccuracy of all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e9c10d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_baseline_augmentation_0.pt 0.79405504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_baseline_augmentation_100.pt 0.9406273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_baseline_augmentation_20.pt 0.90721476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_baseline_augmentation_200.pt 0.94876254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_baseline_augmentation_50.pt 0.9383766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_datasetgan_augmentation_0.pt 0.82673806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_datasetgan_augmentation_100.pt 0.94818574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_datasetgan_augmentation_20.pt 0.9222699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_datasetgan_augmentation_200.pt 0.95033234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_datasetgan_augmentation_50.pt 0.9312889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_no_augmentation_100.pt 0.9353795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_no_augmentation_20.pt 0.86314094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_no_augmentation_200.pt 0.9462622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s2agarwal/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_no_augmentation_50.pt 0.9047231\n"
     ]
    }
   ],
   "source": [
    "model_names = sorted([i for i in os.listdir('Models') if i.endswith('.pt')])\n",
    "for model in model_names:\n",
    "    test_accuracy = test(os.path.join('Models/',model),test_dataloader)\n",
    "    print(model,test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3777a8",
   "metadata": {},
   "source": [
    "### Results - Dice Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f531dbe",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|Train Size| Without Augmentation | Baseline Augmentation| With Augmentation |\n",
    "| :-: | :-: |:-: | :-: |\n",
    "| 0   | -       | 0.794 | 0.826 |\n",
    "| 20  | 0.863  | 0.907 | 0.922  |\n",
    "| 50  | 0.904  | 0.938 | 0.931 |\n",
    "| 100 | 0.935 | 0.940 | 0.948 |\n",
    "| 200 | 0.946 | 0.948 | 0.950 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
